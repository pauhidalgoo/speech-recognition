{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":88948,"databundleVersionId":10213764,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2020 The TensorFlow Authors.","metadata":{"id":"fluF3_oOgkWF"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"AJs7HHFmg1M9","execution":{"iopub.status.busy":"2024-12-23T18:34:52.977508Z","iopub.execute_input":"2024-12-23T18:34:52.977880Z","iopub.status.idle":"2024-12-23T18:34:52.999032Z","shell.execute_reply.started":"2024-12-23T18:34:52.977838Z","shell.execute_reply":"2024-12-23T18:34:52.998112Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Setup\n\nImport necessary modules and dependencies. You'll be using `tf.keras.utils.audio_dataset_from_directory` (introduced in TensorFlow 2.10), which helps generate audio classification datasets from directories of `.wav` files. You'll also need [seaborn](https://seaborn.pydata.org) for visualization in this tutorial.","metadata":{"id":"Go9C3uLL8Izc"}},{"cell_type":"code","source":"!pip install -U -q tensorflow==2.16.1  tensorflow_datasets","metadata":{"id":"hhNW45sjDEDe","outputId":"d138b130-ff81-4d63-9c9f-4fdf42d62bba","execution":{"iopub.status.busy":"2024-12-23T18:34:53.001381Z","iopub.execute_input":"2024-12-23T18:34:53.001735Z","iopub.status.idle":"2024-12-23T18:35:07.138320Z","shell.execute_reply.started":"2024-12-23T18:34:53.001692Z","shell.execute_reply":"2024-12-23T18:35:07.137176Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython import display\n\n# Set the seed value for experiment reproducibility.\nseed = 42\ntf.random.set_seed(seed)\nnp.random.seed(seed)","metadata":{"id":"dzLKpmZICaWN","execution":{"iopub.status.busy":"2024-12-23T18:35:07.142901Z","iopub.execute_input":"2024-12-23T18:35:07.143236Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import the Speech Commands dataset\n\nThis is the [dataset](https://dax-cdn.cdn.appdomain.cloud/dax-tensorflow-speech-commands/1.0.1/data_preview/index.html) that you will be working on. This dataset consists of 65,000 WAV Files and the audio clips were originally collected by Google, and recorded by volunteers in uncontrolled locations around the world.","metadata":{"id":"yR0EdgrLCaWR"}},{"cell_type":"code","source":"DATASET_PATH = '/kaggle/input/tvd-2024-reconocimiento-de-comandos-de-voz/train/train/audio'\n\ndata_dir = pathlib.Path(DATASET_PATH)","metadata":{"id":"2-rayb7-3Y0I","outputId":"5fe3cf95-e37d-4e31-8570-d210fc4a3f06","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset's audio clips are stored in 31 folders corresponding to each speech command. \n\n- 20 core words: `yes`, `no`, `up`, `down`, `left`,`right`, `on`, `off`, `stop`, `go`, `zero`, `one`,`two`, `three`, `four`, `five`,`six`, `seven`, `eight`, `nine`,   \n\n- 10 auxiliary words: `bed`, `bird`, `cat`, `dog`, `happy`, `house`, `marvin`, `sheila`, `tree` and `wow`\n  \n- And background noise: `doing_the_dishes`, `dude_miaowing`, `exercise_bike`, `pink_noise`, `running_tap`, and `white_noise`.","metadata":{"id":"BgvFq3uYiS5G"}},{"cell_type":"code","source":"commands = np.array(tf.io.gfile.listdir(str(data_dir)))\ncommands = commands[(commands != 'README.md') & (commands != '.DS_Store')]\nprint('Commands:', commands)","metadata":{"id":"70IBxSKxA1N9","outputId":"d0f2892a-c56b-4c20-ae85-213df878b0cf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Divided into directories this way, you can easily load the data using `keras.utils.audio_dataset_from_directory`.\n\nThe audio clips are 1 second or less at 16kHz. The `output_sequence_length=16000` pads the short ones to exactly 1 second (and would trim longer ones) so that they can be easily batched.","metadata":{"id":"TZ7GJjDvHqtt"}},{"cell_type":"code","source":"train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n    directory=data_dir,\n    batch_size=64,\n    validation_split=0.2,\n    seed=0,\n    output_sequence_length=16000,\n    subset='both')\n\nlabel_names = np.array(train_ds.class_names)\nprint()\nprint(\"label names:\", label_names)","metadata":{"id":"mFM4c3aMC8Qv","outputId":"30b18663-bb1f-4e29-dc00-fa67fe84f02a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset now contains batches of audio clips and integer labels. The audio clips have a shape of `(batch, samples, channels)`.","metadata":{"id":"cestp83qFnU5"}},{"cell_type":"code","source":"train_ds.element_spec","metadata":{"id":"3yU6SQGIFb3H","outputId":"65114e1e-e366-4ef8-94fd-5fb9fa209b51","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This dataset only contains single channel audio, so use the `tf.squeeze` function to drop the extra axis:","metadata":{"id":"ppG9Dgq2Ex8R"}},{"cell_type":"code","source":"def squeeze(audio, labels):\n  audio = tf.squeeze(audio, axis=-1)\n  return audio, labels\n\ntrain_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\nval_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)","metadata":{"id":"Xl-tnniUIBlM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `utils.audio_dataset_from_directory` function only returns up to two splits. It's a good idea to keep a test set separate from your validation set.\nIdeally you'd keep it in a separate directory, but in this case you can use `Dataset.shard` to split the validation set into two halves. Note that iterating over **any** shard will load **all** the data, and only keep its fraction.","metadata":{"id":"DtsCSWZN5ILv"}},{"cell_type":"code","source":"test_ds = val_ds.shard(num_shards=2, index=0)\nval_ds = val_ds.shard(num_shards=2, index=1)","metadata":{"id":"u5UEGsqM5Gss","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for example_audio, example_labels in train_ds.take(1):\n  print(example_audio.shape)\n  print(example_labels.shape)","metadata":{"id":"xIeoJcwJH5h9","outputId":"98686086-9479-49d0-9422-d46c5a507c6d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's plot a few audio waveforms:","metadata":{"id":"voxGEwvuh2L7"}},{"cell_type":"code","source":"label_names[[1,1,3,0]]","metadata":{"id":"dYtGq2zYNHuT","outputId":"894bc199-c9a5-4c56-db4d-553b38ce7edd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nrows = 3\ncols = 3\nn = rows * cols\nfor i in range(n):\n  plt.subplot(rows, cols, i+1)\n  audio_signal = example_audio[i]\n  plt.plot(audio_signal)\n  plt.title(label_names[example_labels[i]])\n  plt.yticks(np.arange(-1.2, 1.2, 0.2))\n  plt.ylim([-1.1, 1.1])","metadata":{"id":"8yuX6Nqzf6wT","outputId":"93670e7d-334e-4df2-be74-0c0504aae10b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Convert waveforms to spectrograms\n\nThe waveforms in the dataset are represented in the time domain. Next, you'll transform the waveforms from the time-domain signals into the time-frequency-domain signals by computing the [short-time Fourier transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) to convert the waveforms to as [spectrograms](https://en.wikipedia.org/wiki/Spectrogram), which show frequency changes over time and can be represented as 2D images. You will feed the spectrogram images into your neural network to train the model.\n\nA Fourier transform (`tf.signal.fft`) converts a signal to its component frequencies, but loses all time information. In comparison, STFT (`tf.signal.stft`) splits the signal into windows of time and runs a Fourier transform on each window, preserving some time information, and returning a 2D tensor that you can run standard convolutions on.\n\nCreate a utility function for converting waveforms to spectrograms:\n\n- The waveforms need to be of the same length, so that when you convert them to spectrograms, the results have similar dimensions. This can be done by simply zero-padding the audio clips that are shorter than one second (using `tf.zeros`).\n- When calling `tf.signal.stft`, choose the `frame_length` and `frame_step` parameters such that the generated spectrogram \"image\" is almost square. For more information on the STFT parameters choice, refer to [this Coursera video](https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe) on audio signal processing and STFT.\n- The STFT produces an array of complex numbers representing magnitude and phase. However, in this tutorial you'll only use the magnitude, which you can derive by applying `tf.abs` on the output of `tf.signal.stft`.","metadata":{"id":"EWXPphxm0B4m"}},{"cell_type":"code","source":"def get_spectrogram(waveform):\n  # Convert the waveform to a spectrogram via a STFT.\n  spectrogram = tf.signal.stft(\n      waveform, frame_length=255, frame_step=128)\n  # Obtain the magnitude of the STFT.\n  spectrogram = tf.abs(spectrogram)\n  # Add a `channels` dimension, so that the spectrogram can be used\n  # as image-like input data with convolution layers (which expect\n  # shape (`batch_size`, `height`, `width`, `channels`).\n  spectrogram = spectrogram[..., tf.newaxis]\n  return spectrogram\n\n\ndef get_mel_spectogram(waveform):\n  # Compute the STFT.\n  stft = tf.signal.stft(\n      waveform, frame_length=255, frame_step=128)\n  magnitude_spectrogram = tf.abs(stft)\n\n  # Define the Mel filter bank.\n  num_spectrogram_bins = magnitude_spectrogram.shape[-1]\n  linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n      num_mel_bins=128,\n      num_spectrogram_bins=num_spectrogram_bins,\n      sample_rate=16000,  # Replace with your sample rate.\n      lower_edge_hertz=80.0, \n      upper_edge_hertz=7600.0)\n\n  # Apply the Mel scale.\n  mel_spectrogram = tf.tensordot(\n      magnitude_spectrogram, linear_to_mel_weight_matrix, axes=[-1, 0])\n  mel_spectrogram.set_shape(magnitude_spectrogram.shape[:-1] + (128,))\n\n  mel_spectrogram = mel_spectrogram[..., tf.newaxis]\n  return mel_spectrogram\n\ndef get_mfcc(waveform, num_mel_bins=128, num_mfccs=13):\n    # Compute the Mel Spectrogram.\n    stft = tf.signal.stft(\n        waveform, frame_length=255, frame_step=128)\n    magnitude_spectrogram = tf.abs(stft)\n\n    num_spectrogram_bins = magnitude_spectrogram.shape[-1]\n    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n        num_mel_bins=num_mel_bins,\n        num_spectrogram_bins=num_spectrogram_bins,\n        sample_rate=16000,  # Replace with your sample rate.\n        lower_edge_hertz=80.0,\n        upper_edge_hertz=7600.0)\n\n    mel_spectrogram = tf.tensordot(\n        magnitude_spectrogram, linear_to_mel_weight_matrix, axes=[-1, 0])\n    mel_spectrogram.set_shape(magnitude_spectrogram.shape[:-1] + (num_mel_bins,))\n\n    # Compute MFCCs.\n    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1e-6)  # Avoid log(0).\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n    mfccs = mfccs[..., :num_mfccs]  # Take the first `num_mfccs` coefficients.\n\n    # Add a channel dimension to make it 4D.\n    mfccs = mfccs[..., tf.newaxis]\n    return mfccs\n\n","metadata":{"id":"_4CK75DHz_OR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, start exploring the data. Print the shapes of one example's tensorized waveform and the corresponding spectrogram, and play the original audio:","metadata":{"id":"5rdPiPYJphs2"}},{"cell_type":"code","source":"for i in range(3):\n  label = label_names[example_labels[i]]\n  waveform = example_audio[i]\n  spectrogram = get_spectrogram(waveform)\n\n  print('Label:', label)\n  print('Waveform shape:', waveform.shape)\n  print('Spectrogram shape:', spectrogram.shape)\n  print('Audio playback')\n  display.display(display.Audio(waveform, rate=16000))","metadata":{"id":"4Mu6Y7Yz3C-V","outputId":"326c5b8c-0797-49ee-e861-b10d10302916","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, define a function for displaying a spectrogram:","metadata":{"id":"xnSuqyxJ1isF"}},{"cell_type":"code","source":"def plot_spectrogram(spectrogram, ax):\n  if len(spectrogram.shape) > 2:\n    assert len(spectrogram.shape) == 3\n    spectrogram = np.squeeze(spectrogram, axis=-1)\n  # Convert the frequencies to log scale and transpose, so that the time is\n  # represented on the x-axis (columns).\n  # Add an epsilon to avoid taking a log of zero.\n  log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n  height = log_spec.shape[0]\n  width = log_spec.shape[1]\n  X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n  Y = range(height)\n  ax.pcolormesh(X, Y, log_spec)","metadata":{"id":"e62jzb36-Jog","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Plot the example's waveform over time and the corresponding spectrogram (frequencies over time):","metadata":{"id":"baa5c91e8603"}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 16000])\n\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title('Spectrogram')\nplt.suptitle(label.title())\nplt.show()","metadata":{"id":"d2_CikgY1tjv","outputId":"32ad0b35-a0d8-401e-fff3-51fe72da2d83","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, create spectrogram datasets from the audio datasets:","metadata":{"id":"GyYXjW07jCHA"}},{"cell_type":"code","source":"def make_spec_ds(ds):\n  return ds.map(\n      map_func=lambda audio,label: (get_spectrogram(audio), label),\n      num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"id":"mAD0LpkgqtQo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_spectrogram_ds = make_spec_ds(train_ds)\nval_spectrogram_ds = make_spec_ds(val_ds)\ntest_spectrogram_ds = make_spec_ds(test_ds)","metadata":{"id":"yEVb_oK0oBLQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Examine the spectrograms for different examples of the dataset:","metadata":{"id":"6gQpAAgMnyDi"}},{"cell_type":"code","source":"for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n  break","metadata":{"id":"EaM2q5aGis-d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n\nfor i in range(n):\n    r = i // cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(example_spectrograms[i].numpy(), ax)\n    ax.set_title(label_names[example_spect_labels[i].numpy()])\n\nplt.show()","metadata":{"id":"QUbHfTuon4iF","outputId":"41adcc38-338b-4f3c-b60f-56a5f4b005c5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build and train the model","metadata":{"id":"z5KdY8IF8rkt"}},{"cell_type":"markdown","source":"Add `Dataset.cache` and `Dataset.prefetch` operations to reduce read latency while training the model:","metadata":{"id":"GS1uIh6F_TN9"}},{"cell_type":"code","source":"train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\nval_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\ntest_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)","metadata":{"id":"fdZ6M-F5_QzY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the model, you'll use a simple convolutional neural network (CNN), since you have transformed the audio files into spectrogram images.\n\nYour `tf.keras.Sequential` model will use the following Keras preprocessing layers:\n\n- `tf.keras.layers.Resizing`: to downsample the input to enable the model to train faster.\n- `tf.keras.layers.Normalization`: to normalize each pixel in the image based on its mean and standard deviation.\n\nFor the `Normalization` layer, its `adapt` method would first need to be called on the training data in order to compute aggregate statistics (that is, the mean and the standard deviation).","metadata":{"id":"rwHkKCQQb5oW"}},{"cell_type":"code","source":"# We provide some function you may find useful for your models\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.ffn = Sequential(\n            [\n                layers.Dense(ff_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, embed_dim):\n        super().__init__()\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, inputs):\n        maxlen = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        position_embeddings = self.pos_emb(positions)\n        token_embeddings = inputs\n        return token_embeddings + position_embeddings","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model, regularizers\nfrom tensorflow.keras.optimizers import AdamW\nimport numpy as np\n\n# Squeeze-and-Excitation Block\ndef squeeze_and_excitation_block(input_tensor, ratio=16):\n    channels = input_tensor.shape[-1]\n    se = layers.GlobalAveragePooling2D()(input_tensor)\n    se = layers.Dense(channels // ratio, activation='relu')(se)\n    se = layers.Dense(channels, activation='sigmoid')(se)\n    return layers.Multiply()([input_tensor, se])\n\n# Enhanced Multi-Head Attention Block\ndef multi_head_attention_block(inputs, embed_dim, num_heads, ff_dim, dropout=0.1):\n    # Layer Normalization before attention\n    normalized_inputs = layers.LayerNormalization(epsilon=1e-6)(inputs)\n    \n    # Multi-Head Attention\n    attention_output = layers.MultiHeadAttention(\n        num_heads=num_heads, \n        key_dim=embed_dim//num_heads,\n        dropout=dropout\n    )(normalized_inputs, normalized_inputs)\n    \n    # Residual Connection\n    attention_output = layers.Add()([inputs, attention_output])\n    \n    # Feed-Forward Network with Residual Connection\n    ffn = layers.Dense(ff_dim, activation=\"gelu\")(attention_output)\n    ffn = layers.Dense(inputs.shape[-1])(ffn)\n    ffn = layers.Dropout(dropout)(ffn)\n    \n    # Residual Connection and Layer Normalization\n    output = layers.LayerNormalization(epsilon=1e-6)(attention_output + ffn)\n    return output\n\n# Hyperparameters (adjust based on your specific dataset)\ninput_shape = example_spectrograms.shape[1:]\nprint('Input shape:', input_shape)\nnum_labels = len(label_names)\nembed_dim = 256\nnum_heads = 8\nff_dim = 512\ndropout_rate = 0.15\n\n# Learning Rate Schedule\ntotal_training_steps = 10000  # Adjust based on your dataset and epochs\nlr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=1e-3, \n    decay_steps=total_training_steps\n)\n\ndef build_model(input_shape=(128, 128, 1), num_labels=31, dropout_rate=0.2, lr_schedule=1e-3):\n    from tensorflow.keras import layers, Model\n    from tensorflow.keras.optimizers import AdamW\n    \n    # Input Layer\n    inputs = layers.Input(shape=input_shape)\n    \n    # Input Preprocessing\n    x = layers.Resizing(128, 128)(inputs)  # Resize to 128x128 as per table\n    x = layers.Normalization()(x)  # Normalize inputs\n\n    # First Convolutional Block\n    x = layers.Conv2D(256, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n    x = layers.LayerNormalization()(x)  # LayerNormalization as per table\n    x = layers.Dropout(dropout_rate)(x)\n\n    # Second Convolutional Block\n    x = layers.Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n    x = layers.LayerNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    # Third Convolutional Block\n    x = layers.Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n    x = layers.LayerNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    # Fourth Convolutional Block\n    x = layers.Conv2D(64, kernel_size=3, strides=2, activation='relu', padding='same')(x)\n    x = layers.LayerNormalization()(x)\n    x = layers.Dropout(dropout_rate)(x)\n\n    # MaxPooling Layer\n    x = layers.MaxPooling2D(pool_size=2, strides=2)(x)\n\n    # Flatten the feature maps\n    x = layers.Flatten()(x)\n\n    # Fully Connected Layer (Dense Layer)\n    outputs = layers.Dense(num_labels, activation=\"softmax\")(x)\n\n    # Create Model\n    model = Model(inputs, outputs)\n\n    # Compile with AdamW Optimizer and Cosine Decay Learning Rate\n    model.compile(\n        optimizer=AdamW(\n            learning_rate=lr_schedule, \n            weight_decay=1e-5\n        ),\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model\n\n\n# Create and Compile Model\nmodel = build_model()\nmodel.summary()","metadata":{"id":"ALYz7PFCHblP","outputId":"6b58ca96-8b72-4a87-a08f-f0a41d598b88","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"initial_learning_rate = 0.003\ndecay_steps = 1000  # Number of steps for decay\nalpha = 0.1  # Minimum learning rate value as a fraction of the initial_learning_rate\n\n# Create the cosine decay schedule\ncosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n    initial_learning_rate=initial_learning_rate,\n    decay_steps=decay_steps,\n    alpha=alpha\n)\n\n\n# Use the schedule with the Adam optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=cosine_decay)\n\n# Compile the model\nmodel.compile(\n    optimizer=optimizer,\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy']\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Configure the Keras model with the Adam optimizer and the cross-entropy loss:","metadata":{"id":"de52e5afa2f3"}},{"cell_type":"code","source":"model.summary()","metadata":{"id":"wFjj7-EmsTD-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train the model over 10 epochs for demonstration purposes:","metadata":{"id":"f42b9e3a4705"}},{"cell_type":"code","source":"EPOCHS = 30\nhistory = model.fit(\n    train_spectrogram_ds,\n    validation_data=val_spectrogram_ds,\n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=30),\n)","metadata":{"id":"ttioPJVMcGtq","outputId":"8ed2586a-f90a-490e-ff3a-c3312544d3e0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's plot the training and validation loss curves to check how your model has improved during training:","metadata":{"id":"gjpCDeQ4mUfS"}},{"cell_type":"code","source":"metrics = history.history\nplt.figure(figsize=(16,6))\nplt.subplot(1,2,1)\nplt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.ylim([0, max(plt.ylim())])\nplt.xlabel('Epoch')\nplt.ylabel('Loss [CrossEntropy]')\n\nplt.subplot(1,2,2)\nplt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\nplt.legend(['accuracy', 'val_accuracy'])\nplt.ylim([0, 100])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy [%]')","metadata":{"id":"nzhipg3Gu2AY","outputId":"f19aa9c8-21a6-4622-f41d-94aaff16516b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate the model performance\n\nRun the model on the test set and check the model's performance:","metadata":{"id":"5ZTt3kO3mfm4"}},{"cell_type":"code","source":"model.evaluate(test_spectrogram_ds, return_dict=True)","metadata":{"id":"FapuRT_SsWGQ","outputId":"33240e49-1079-4168-d691-804f371ec9bf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Display a confusion matrix\n\nUse a [confusion matrix](https://developers.google.com/machine-learning/glossary#confusion-matrix) to check how well the model did classifying each of the commands in the test set:\n","metadata":{"id":"en9Znt1NOabH"}},{"cell_type":"code","source":"y_pred = model.predict(test_spectrogram_ds)","metadata":{"id":"5Y6vmWWQuuT1","outputId":"1c909499-5c85-46c2-af10-7f216f591f59","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = tf.argmax(y_pred, axis=1)","metadata":{"id":"d6F0il82u7lW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)","metadata":{"id":"vHSNoBYLvX81","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx,\n            xticklabels=label_names,\n            yticklabels=label_names,\n            annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()","metadata":{"id":"LvoSAOiXU3lL","outputId":"52fcd49c-ae7a-43d4-9e7b-9a27d0d9b9fe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generate the submission file\n\nThe code below produces the `submission.csv` file, which you are required to submit for the competition. **Please do not make any changes in this cell**. ","metadata":{"id":"mQGi_mzPcLvl"}},{"cell_type":"code","source":"import pandas as pd\n\nl = [[i,j] for i,j in zip(list(range(1,len(y_pred)+1)), y_pred.numpy().tolist())]\n\ndf = pd.DataFrame(l, columns=['ID','labels'])\n\ndf.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}